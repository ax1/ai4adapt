- [x] Test higher learning rate. Now that rewards are atomic, we can increase to ensure that good ones are learnt faster (in incremental there were a bunch of good ones depending of the episode because they were with the good one, but in this case, only the valid one is rewarded). Nope, reduced to not-default but low value. In case of limited iterations the agent stucks in partial solutions for quite a long, because it tests all the actions around an observation. Also problems where high noise even if probabilistic successes tend to the normal observations. keep LR low for non-simulated envs.
- [ ] Try sensitivity going back to default params. We do no need so many policy evaluations on each block with this atomic format. Not fully tested but after experiments, prediction is 2X iterations more expensive, with the advantage of no tuning required.
- [ ] Learn to resolve machine 3 (even if not reward.win, but at least reward.survive or similar reward) and also learn to do-nothing after resolved (eg: 300), because this will be useful in the recommendation mode of the agent when the attack is stopped but the RL is continuosly running.
- [ ] current proxmox backup blocks the api calls. Schedule has been reprogrammed to weekly SUN 01:00. But it is better to add in reset another lock checker to wait for proxmox to be ready instead of stopping the training.
- [ ] Stop caldera attack when many failed. Otherwise if first defense action is shutdown first machine, the attack cannot evolve but since we have sequential, the attack reaches 3rd machine and a 001 is returned.