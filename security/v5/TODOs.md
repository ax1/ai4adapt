- [x] Test higher learning rate. Now that rewards are atomic, we can increase to ensure that good ones are learnt faster (in incremental there were a bunch of good ones depending of the episode because they were with the good one, but in this case, only the valid one is rewarded). Nope, reduced to not-default but low value. In case of limited iterations the agent stucks in partial solutions for quite a long, because it tests all the actions around an observation. Also problems where high noise even if probabilistic successes tend to the normal observations. keep LR low for non-simulated envs.
- [x] Try sensitivity going back to default params. We do no need so many policy evaluations on each block with this atomic format. Not fully tested but after experiments, prediction is 2X iterations more expensive, with the advantage of no tuning required. DONE, given so short of iterations, using defaults require much more steps to resolve
- [x] Learn to resolve machine 3 (even if not reward.win, but at least reward.survive or similar reward) and also learn to do-nothing after resolved (eg: 300), because this will be useful in the recommendation mode of the agent when the attack is stopped but the RL is continuosly running. TESTED but UNRESOLVED yet, we cannot do it unless we add temp observation.
- [x] current proxmox backup blocks the api calls. Schedule has been reprogrammed to weekly SUN 01:00. But it is better to add in reset another lock checker to wait for proxmox to be ready instead of stopping the training. Done in the env but proxmox still with problems, so no options but to migrate to another Proxmox under our full control. 
- [x] Stop caldera attack when many failed. Otherwise if first defense action is shutdown first machine, the attack cannot evolve but since we have sequential, the attack reaches 3rd machine and a 001 is returned. NOT DONE, in the end it is better to see that executing that action at \[000\] should not be invoked, so it is ok to train for that case even if all the time is the same observation (and we do not want to add optimizations in code only for performance and penalizing environment abstraction).