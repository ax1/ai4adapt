# TODOs

- [x] Decide best lib for DRL at this first stage. Since we are putting all the weight on the real environment, most of the RL libs may fit, but discard obsolete ones (some requires downgrade to tensorflow 1 to run som are created for gym and not updated to gymnasium terminate/truncate etc). Don gymnasion and SB3 are fine.
- Check best policies from available. Note that SP3 remove some ones available in obsolete SB (https://stable-baselines3.readthedocs.io/en/master/guide/migration.html#breaking-changes). MlpPlocy CNNPolicy and MultiInput Policy where observation is `Space`  type (array) for MLP and `Dict` type for multiInput. Since some envs use plain ist of int or float arr vals, for now observations will be like that MLPpolicy is completely fine.
- [x] If SB3 selected check why sb3.make_vec_env returns the old gym format (done field) instead of the new one (terminated, truncated fields). By looking at the source code https://stable-baselines3.readthedocs.io/en/master/_modules/stable_baselines3/common/env_util.html term, truncated should appear if the env we use implements it. They use the 'dones' field as array of [term,trunc]
- [x] Decide Algorithm. PPO was best candidate but all DRL suffer from fast-convergence even if we set final rewards very high. For now, non-drl algorithms solve ultra-fast convergence (at the expense on not very academic), but on real env training step time will be high and we need RL to converge FAST. See the SImpleCombination example folder.  For now PPL, in case the real env is too time consuming, prepare a dedicated fast convergence algo.
- [x] Create generic remote call to target where sensing will be retrieved. For now, this app will be inside the machine with caldera, or remote, but check if we can call remotely caldera actions.
- [x] Start witn status sensing (depending on secondary variables in the "state" we can infer a status value for training, later we will add more state data. Removed, now we use crafted state-machine like observations from events, instead of general purpose observations.
- [x] Split the defense to the defense execute (we've agreed to use the caldera actions for that).
- [ ] Reduce time. Unless a script to have all the actions (so we can test defenses are ok). Then, in caldera operations either execute the script with args 8eg: 1 4 8 execute only those in the script) or execute loop actions with an argument on each execution (to prevent create operations for all the steps and also to handle to reduce the time of each episode for training). NOT POSSOBLE FOR NOW.


# TBD ASAP
- [x] Test same PPO as lunar but with the sec_env (it should work with minimal changes). Results, ppo is better to find the real solution, but they need similar huge number of iterations to learn 
- [ ] Create a vm or docker with simple attack+defense set and create a external monitor link (probably as docker execute). This should work as generic Sec real event for bare test of the effectiveness of the RL part. THIS KEEPS SEPARATION BETWEEN THE RL EFFECTIVENESS AND THE REAL ENVIRONMENT COMPLEXITY (with a simpler but real env we can avoid the trap of adding much custom real environment and tools instead of focusing on the real training effectiveness).
- [ ] Check caldera how to access data internally externally. Talking with OscarL we agree that we can simplify the operations as an sh script having the defenses procedures (installation+execution individually) and call it with params, so for defense [1, 4, 7] the operations in the caldera could be the same action but with different start parameters (quite easy to maintain, and once learned optimal solution we can set up the caldera workflow manually to take advantage of the visualization).
- [x] Automate vm life-cycle. Done.