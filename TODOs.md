# TODOs

- [ ] Decide best lib for DRL at this first stage. Since we are putting all the weight on the real environment, most of the RL libs may fit, but discard obsolete ones (som requires downgrade to tensorflow 1 to run som are created for gym and not updated to gymnasium terminate/truncate etc). 
- Check best policies from available. Note that SP3 remove some ones available in obsolete SB (https://stable-baselines3.readthedocs.io/en/master/guide/migration.html#breaking-changes). MlpPlocy CNNPolicy and MultiInput Policy where observation is `Space`  type (array) for MLP and `Dict` type for multiInput.
- [ ] If SB3 selected check why sb3.make_vec_env returns the old gym format (done field) instead of the new one (terminated, truncated fields). By looking at the source code https://stable-baselines3.readthedocs.io/en/master/_modules/stable_baselines3/common/env_util.html term, truncated should appear if the env we use implements it.
- [ ] Decide Algorithm. PPO for now seems the right choice.
- [ ] Create generic remote call to target where sensing will be retrieved. For now, this app will be inside the machine with caldera, or remote, but check if we can call remotely caldera actions.
- [ ] Start witn status sensing (depending on secondary variables in the "state" we can infer a status value for training, later we will add more state data.
- [ ] Split the defense to the defense execute (we've agreed to use the caldera actions for that).
- [ ] Reduce time. Unless a script to have all the actions (so we can test defenses are ok). Then, in caldera operations either execute the script with args 8eg: 1 4 8 execute only those in the script) or execute loop actions with an argument on each execution (to prevent create operations for all the steps and also to handle to reduce the time of each episode for training).

# TBD ASAP
- [ ] Test same PPO as lunar but with the sec_env (it should work with minimal changes).
- [ ] Create a vm or docker with simple attack+defense set and create a external monitor link (probably as docker execute). This should work as generic Sec real event for bare test of the effectiveness of the RL part. THIS KEEPS SEPARATION BETWEEN THE RL EFFECTIVENESS AND THE REAL ENVIRONMENT COMPLEXITY (with a simpler but real env we can avoid the trap of adding much custom real environment and tools instead of focusing on the real training effectiveness).
- [ ] Check caldera how to access data internally externally. Talking with OscarL we agree that we can simplify the operations as an sh script having the defenses procedures (installation+execution individually) and call it with params, so for defense [1, 4, 7] the operations in the caldera could be the same action but with different start parameters (quite easy to maintain, and once learned optimal solution we can set up the caldera workflow manually to take advantage of the visualization).